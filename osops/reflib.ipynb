{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671c947e-54b4-44d2-a76f-0af8de9ad908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T20:51:28.675700Z",
     "iopub.status.busy": "2022-12-16T20:51:28.675511Z",
     "iopub.status.idle": "2022-12-16T20:51:28.678600Z",
     "shell.execute_reply": "2022-12-16T20:51:28.678318Z",
     "shell.execute_reply.started": "2022-12-16T20:51:28.675690Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from collections import OrderedDict as od\n",
    "\n",
    "import Bio.Entrez as Entrez\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61505939-dbb5-4962-b4a0-64b54df47534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T04:46:57.735181Z",
     "iopub.status.busy": "2022-12-07T04:46:57.734388Z",
     "iopub.status.idle": "2022-12-07T04:46:57.752264Z",
     "shell.execute_reply": "2022-12-07T04:46:57.751350Z",
     "shell.execute_reply.started": "2022-12-07T04:46:57.735121Z"
    },
    "tags": []
   },
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8feaf4aa-67eb-4702-9837-4819f9f5de0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T22:39:03.407126Z",
     "iopub.status.busy": "2022-12-16T22:39:03.406371Z",
     "iopub.status.idle": "2022-12-16T22:39:03.452377Z",
     "shell.execute_reply": "2022-12-16T22:39:03.451801Z",
     "shell.execute_reply.started": "2022-12-16T22:39:03.407065Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linklib(parent, check=None, parse_pdf=False, mode: str = \"rename\", verbose=False):\n",
    "    \"\"\"Link to each file at each level above it in the file hierarchy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent : str\n",
    "        The path to the parent directory.\n",
    "    check : str\n",
    "        The path to the directory that checks all file names for\n",
    "        duplicates with non-identical inodes.\n",
    "    mode : str\n",
    "        Determines behavior if a symbolic link already exists.\n",
    "        \"stop\"\n",
    "            An error is raised and the function stops running.\n",
    "        \"skip\"\n",
    "            The function skips to the next file and returns a list\n",
    "            of src paths it failed to create links to.\n",
    "        \"rename\"\n",
    "            The file is renamed with a letter suffix. E.g. if the\n",
    "            link file1.py exists, the code would try to create\n",
    "            file1a.py. If file1a.py exists, it would try to create\n",
    "            file1b.py, etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : OrderedDict\n",
    "        \"renamed_files\" : OrderedDict\n",
    "        \"failed_lookups\" : list\n",
    "        \"duplicates\" : list\n",
    "        \"files_renamed\" : int\n",
    "        \"symlinks_created\" : int\n",
    "        \"symlinks_removed\" : int\n",
    "    \"\"\"\n",
    "    if mode not in (\"rename\", \"skip\", \"stop\"):\n",
    "        raise ValueError\n",
    "    if check is not None and not op.exists(check):\n",
    "        raise FileNotFoundError(check)\n",
    "\n",
    "    # Find and remove broken symlinks.\n",
    "    symlinks_removed = 0\n",
    "    symlinks_removed += rm_symlinks(parent)\n",
    "    if verbose and symlinks_removed > 0:\n",
    "        print(\"Removed {} symlinks before new link creation.\".format(symlinks_removed))\n",
    "\n",
    "    # Get regular files (not dirs or symlinks).\n",
    "    files = find_files_at_depth(\n",
    "        parent,\n",
    "        show_files=True,\n",
    "        show_symlinks=False,\n",
    "        show_hidden_files=False,\n",
    "        keep_ext=[\".pdf\"],\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"Found {} PDF files in {}\".format(len(files), parent))\n",
    "\n",
    "    # Create a symlink to each file in the master directory,\n",
    "    # checking for and handling duplicate file names along the way.\n",
    "    renamed_files = od([])\n",
    "    failed_lookups = []\n",
    "    duplicates = []\n",
    "    files_renamed = 0\n",
    "    symlinks_created = 0\n",
    "    for src in files:\n",
    "        srcdir, base = op.split(src)\n",
    "        if check is not None:\n",
    "            # Rename the paper by scraping the PDF and searching for\n",
    "            # metadata in PubMed.\n",
    "            if parse_pdf:\n",
    "                try:\n",
    "                    # ================\n",
    "                    pdfinfo = rename_paper(src, check, inplace=True, verbose=verbose)\n",
    "                    return src, pdfinfo\n",
    "                    # ================\n",
    "                    newsrc = rename_paper(src, check, inplace=True, verbose=verbose)\n",
    "                    renamed_files[src] = newsrc\n",
    "                    src = newsrc\n",
    "                    srcdir, base = op.split(src)\n",
    "                    files_renamed += 1\n",
    "                except:\n",
    "                    failed_lookups.append(src)\n",
    "                    continue\n",
    "\n",
    "            # Check symlink.\n",
    "            dst = op.join(check, base)\n",
    "            if op.exists(dst):\n",
    "                if same_file(src, dst):\n",
    "                    pass\n",
    "                elif mode == \"stop\":\n",
    "                    raise FileExistsError(dst)\n",
    "                elif mode == \"skip\":\n",
    "                    duplicates.append(src)\n",
    "                    continue\n",
    "                elif mode == \"rename\":\n",
    "                    newsrc = get_unique_name(src, [check, srcdir])\n",
    "                    os.rename(src, newsrc)\n",
    "                    renamed_files[src] = newsrc\n",
    "                    src = newsrc\n",
    "                    dst = op.join(check, op.basename(src))\n",
    "                    os.symlink(src, dst)\n",
    "                    files_renamed += 1\n",
    "                    symlinks_created += 1\n",
    "            else:\n",
    "                os.symlink(src, dst)\n",
    "                symlinks_created += 1\n",
    "\n",
    "    # Find and remove broken symlinks.\n",
    "    symlinks_removed += rm_symlinks(parent)\n",
    "\n",
    "    # Create links at each level from parent to each file's location.\n",
    "    symlinks_created += fill_symlinks(parent)\n",
    "\n",
    "    # Print runtime details.\n",
    "    if verbose:\n",
    "        if parse_pdf:\n",
    "            print(\"Failed to parse {} files\".format(len(failed_lookups)))\n",
    "        if mode == \"skip\":\n",
    "            print(\"Skipped {} duplicate files\".format(len(duplicates)))\n",
    "        print(\"Renamed {} files\".format(files_renamed))\n",
    "        print(\"Created {} symlinks\".format(symlinks_created))\n",
    "        print(\"Removed {} broken symlinks\".format(symlinks_removed))\n",
    "\n",
    "    output = od(\n",
    "        [\n",
    "            (\"renamed_files\", renamed_files),\n",
    "            (\"failed_lookups\", failed_lookups),\n",
    "            (\"duplicates\", duplicates),\n",
    "            (\"files_renamed\", files_renamed),\n",
    "            (\"symlinks_created\", symlinks_created),\n",
    "            (\"symlinks_removed\", symlinks_removed),\n",
    "        ]\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "def alphabet_generator():\n",
    "    current_letter = \"a\"\n",
    "    while True:\n",
    "        yield current_letter\n",
    "        current_letter = next_letter(current_letter)\n",
    "\n",
    "\n",
    "def next_letter(current_letter):\n",
    "    if current_letter == \"z\":\n",
    "        return \"aa\"\n",
    "    elif len(current_letter) == 1:\n",
    "        return chr(ord(current_letter) + 1)\n",
    "    else:\n",
    "        first_letter, second_letter = current_letter\n",
    "        if second_letter == \"z\":\n",
    "            return chr(ord(first_letter) + 1) + \"a\"\n",
    "        else:\n",
    "            return first_letter + chr(ord(second_letter) + 1)\n",
    "\n",
    "\n",
    "def find_files_at_depth(\n",
    "    parent,\n",
    "    depth=None,\n",
    "    show_files=True,\n",
    "    show_symlinks=False,\n",
    "    show_hidden_files=True,\n",
    "    show_hidden_dirs=True,\n",
    "    keep_ext=[],\n",
    "):\n",
    "    \"\"\"Return all files in parent and its nested subdirs up to depth.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent : str\n",
    "        The path to the parent directory.\n",
    "    depth : int, optional\n",
    "        The depth of subdirectories to search.\n",
    "        If None, all files in the hierarchy are found.\n",
    "        If 0, only files in parent are found.\n",
    "        If 1, only files in parent and its immediate subdirectories are\n",
    "        found.\n",
    "        Etc.\n",
    "    show_files : bool, optional\n",
    "        Whether to return regular files (i.e. not symlinks). The default\n",
    "        value is True.\n",
    "    show_symlinks : bool, optional\n",
    "        Whether to return symlinks. The default value is False.\n",
    "    show_hidden_files : bool, optional\n",
    "        Whether to return hidden files. The default value is True.\n",
    "    show_hidden_dirs : bool, optional\n",
    "        Whether to search for files in hidden directories. The default\n",
    "        value is True.\n",
    "    keep_ext : list, optional\n",
    "        A list of filename extensions. Only files with these extensions\n",
    "        will be returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of all the files found at the specified depth.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "\n",
    "    # Walk through the directory tree and collect all files at the specified depth\n",
    "    for root, dirs, filenames in os.walk(parent):\n",
    "        # Skip hidden directories if show_hidden_dirs is False\n",
    "        if not show_hidden_dirs:\n",
    "            dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "        if depth is None or root.count(op.sep) - parent.count(op.sep) == depth:\n",
    "            # Skip hidden files if hidden_files is False\n",
    "            if not show_hidden_files:\n",
    "                filenames = [f for f in filenames if not f.startswith(\".\")]\n",
    "            # Skip regular files if show_files is False\n",
    "            if not show_files:\n",
    "                filenames = [f for f in filenames if op.islink(op.join(root, f))]\n",
    "            # Skip symlinks if show_symlinks is False\n",
    "            if not show_symlinks:\n",
    "                filenames = [f for f in filenames if not op.islink(op.join(root, f))]\n",
    "            # Keep only files with the specified extensions\n",
    "            if keep_ext:\n",
    "                keep_ext = [\n",
    "                    ext if ext.startswith(\".\") else \".{}\".format(ext)\n",
    "                    for ext in keep_ext\n",
    "                ]\n",
    "                filenames = [f for f in filenames if op.splitext(f)[1] in keep_ext]\n",
    "            for f in filenames:\n",
    "                files.append(op.join(root, f))\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def same_file(file1, file2):\n",
    "    \"\"\"Return whether two files are the same.\"\"\"\n",
    "    try:\n",
    "        return os.stat(file1).st_ino == os.stat(file2).st_ino\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def rm_symlinks(parent, assert_same_basename=True):\n",
    "    \"\"\"Recursively remove broken symlinks in parent.\n",
    "\n",
    "    Returns the number of symlinks removed.\n",
    "    \"\"\"\n",
    "    # Find and remove broken symlinks.\n",
    "    n_removed = 0\n",
    "    symlinks = find_files_at_depth(parent, show_files=False, show_symlinks=True)\n",
    "    for link in symlinks:\n",
    "        if op.islink(link) and not op.exists(link):\n",
    "            os.remove(link)\n",
    "            n_removed += 1\n",
    "        elif assert_same_basename and not (\n",
    "            op.basename(link) == op.basename(op.realpath(link))\n",
    "        ):\n",
    "            os.remove(link)\n",
    "            n_removed += 1\n",
    "    return n_removed\n",
    "\n",
    "\n",
    "def fill_symlinks(parent):\n",
    "    \"\"\"Recursively link from parent down to each file's bottom dir.\n",
    "\n",
    "    Returns the number of symlinks created.\n",
    "    \"\"\"\n",
    "    n_created = 0\n",
    "    files = find_files_at_depth(\n",
    "        parent,\n",
    "        show_files=True,\n",
    "        show_symlinks=False,\n",
    "        show_hidden_files=False,\n",
    "        keep_ext=[\".pdf\"],\n",
    "    )\n",
    "    for src in files:\n",
    "        srcdir, base = op.split(src)\n",
    "        cwd = srcdir\n",
    "        moveon = False\n",
    "        while not moveon:\n",
    "            moveon = cwd == parent\n",
    "            dst = op.join(cwd, base)\n",
    "            if not op.exists(dst):\n",
    "                os.symlink(src, dst)\n",
    "                n_created += 1\n",
    "            cwd = op.dirname(cwd)\n",
    "    return n_created\n",
    "\n",
    "\n",
    "def get_unique_name(src, check):\n",
    "    \"\"\"Return a unique filename checking src file against check dirs.\"\"\"\n",
    "    srcdir = op.dirname(src)\n",
    "    base = \"{}.pdf\".format(op.splitext(op.basename(src))[0])\n",
    "    gen = alphabet_generator()\n",
    "    if isinstance(check, str):\n",
    "        check = [check]\n",
    "    for _check in check:\n",
    "        dst = op.join(_check, base)\n",
    "        if op.exists(dst) and not same_file(src, dst):\n",
    "            name, ext = op.splitext(base)\n",
    "            while op.exists(dst):\n",
    "                base = name + next(gen) + ext\n",
    "                dst = op.join(_check, base)\n",
    "    return op.join(srcdir, base)\n",
    "\n",
    "\n",
    "def rename_paper(infile, check, inplace=False, verbose=False):\n",
    "    \"\"\"Rename paper by looking up author and publication year.\n",
    "    Based on last name of the first author and year of publication.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    infile : str\n",
    "        The path to the file to be renamed.\n",
    "    check : str\n",
    "        The path to the directory that checks all file names for\n",
    "        duplicates with non-identical inodes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outfile : str or None\n",
    "        Path to the renamed file. Outfile is infile if no renaming was\n",
    "        needed, or is None if the PubMed search failed.\n",
    "    \"\"\"\n",
    "    # Scrape document info from the PDF.\n",
    "    pdf_info = PdfFileReader(open(infile, \"rb\")).getDocumentInfo()\n",
    "\n",
    "    # Create a Pubmed query to find the article.\n",
    "    qry = \"\"\n",
    "    if \"/doi\" in pdf_info and pdf_info[\"/doi\"]:\n",
    "        qry += \"{}[aid]\".format(pdf_info[\"/doi\"])\n",
    "    else:\n",
    "        if \"/Author\" in pdf_info and pdf_info[\"/Author\"]:\n",
    "            qry += \"{}[Author] AND \".format(pdf_info[\"/Author\"])\n",
    "        if \"/Title\" in pdf_info and pdf_info[\"/Title\"]:\n",
    "            qry += \"{}[Title] AND \".format(pdf_info[\"/Title\"])\n",
    "        if \"/Subject\" in pdf_info and pdf_info[\"/Subject\"]:\n",
    "            if pdf_info[\"/Subject\"].find(\",\") > 0:\n",
    "                journal = pdf_info[\"/Subject\"].split(\",\")[0]\n",
    "                qry += \"{}[Journal] AND \".format(journal)\n",
    "            # year = re.findall(\"(\\d\\d\\d\\d)\", pdf_info[\"/Subject\"])\n",
    "            # if year:\n",
    "            #     year = int(year[0])\n",
    "            #     qry += \"{}:{}[Publication Date] AND \".format(year - 1, year + 1)\n",
    "        if qry.endwith(\" AND \"):\n",
    "            qry = qry[:-5]\n",
    "\n",
    "    # Run the query and create a preliminary name for the paper if\n",
    "    # PubMed finds a single matching result.\n",
    "    id_list = pubmed_search(qry)[\"IdList\"]\n",
    "    if verbose:\n",
    "        print(\"Found {} results\".format(len(id_list)))\n",
    "    if len(id_list) == 1:\n",
    "        paper = fetch_details(id_list[:1])[\"PubmedArticle\"][0]\n",
    "        try:\n",
    "            name = \"{}{}\".format(\n",
    "                paper[\"MedlineCitation\"][\"Article\"][\"AuthorList\"][0][\"LastName\"],\n",
    "                paper[\"MedlineCitation\"][\"Article\"][\"ArticleDate\"][0][\"Year\"],\n",
    "            )\n",
    "        except (KeyError, TypeError) as err:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Failed to find author name and publication year in PubMed result\"\n",
    "                )\n",
    "            return None\n",
    "\n",
    "    # Check the name against the database, and get a unique name if\n",
    "    # needed.\n",
    "    outfile = op.join(op.dirname(infile), \"{}.pdf\".format(name))\n",
    "    outfile = get_unique_name(src, check)\n",
    "\n",
    "    # Rename the paper.\n",
    "    if inplace and not (infile == outfile):\n",
    "        os.rename(iverbosele, outfile)\n",
    "        if verbose:\n",
    "            print(\"Renamed {} to {}\".format(infile, outfile))\n",
    "\n",
    "    return outfile\n",
    "\n",
    "\n",
    "def pubmed_search(query):\n",
    "    Entrez.email = \"your.email@example.com\"\n",
    "    handle = Entrez.esearch(\n",
    "        db=\"pubmed\", sort=\"relevance\", retmax=\"20\", retmode=\"xml\", term=query\n",
    "    )\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    ids = \",\".join(id_list)\n",
    "    Entrez.email = \"your.email@example.com\"\n",
    "    handle = Entrez.efetch(db=\"pubmed\", retmode=\"xml\", id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879b8e4-6513-4109-94e7-e83b04bf2c58",
   "metadata": {},
   "source": [
    "# Execute library linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b135e09-329b-49bc-b948-d024f2d694b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-16T22:39:25.709015Z",
     "iopub.status.busy": "2022-12-16T22:39:25.708528Z",
     "iopub.status.idle": "2022-12-16T22:39:25.962995Z",
     "shell.execute_reply": "2022-12-16T22:39:25.962556Z",
     "shell.execute_reply.started": "2022-12-16T22:39:25.708969Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 186 PDF files in /Users/dschonhaut/Box/library\n",
      "Renamed 0 files\n",
      "Created 0 symlinks\n",
      "Removed 0 broken symlinks\n"
     ]
    }
   ],
   "source": [
    "# Organize file names and links in library.\n",
    "lib = \"/Users/dschonhaut/Box/library\"\n",
    "parent = lib\n",
    "check = op.join(lib, \".all\")\n",
    "parse_pdf = False\n",
    "mode = \"rename\"\n",
    "verbose = True\n",
    "# ------------------------\n",
    "\n",
    "# renamed_files, failed_lookups, duplicates = linklib(parent, check, lookup_papers, mode, verbose)\n",
    "output = linklib(parent, check=check, parse_pdf=parse_pdf, mode=mode, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277b065-8b93-42de-bd58-3e78ea3654ed",
   "metadata": {},
   "source": [
    "# Parse PDF for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "393e9d66-30b5-43c3-adf1-076d935e2152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40085ee0-2b98-4872-a2bd-619ae8ede0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "infile = op.join(lib, \"ndd\", \"cohorts\", \"adni\", \"1-s2.0-S0197458022002007-main.pdf\")\n",
    "pdf_toread = PdfFileReader(open(infile, \"rb\"))\n",
    "pdf_info = pdf_toread.getDocumentInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be22747e-624d-4374-948c-9527181bbbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Genome-wide association study of brain tau deposition as measured by <sup>18</sup>F-flortaucipir positron emission tomography imaging.\n"
     ]
    }
   ],
   "source": [
    "qry = \"{}[aid]\".format(pdf_info[\"/doi\"])\n",
    "results = pubmed_search(qry)\n",
    "id_list = results[\"IdList\"]\n",
    "papers = fetch_details(id_list)\n",
    "for i, paper in enumerate(papers[\"PubmedArticle\"]):\n",
    "    print(\"{}) {}\".format(i + 1, paper[\"MedlineCitation\"][\"Article\"][\"ArticleTitle\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b587762-bddb-4f31-9922-b01bf5eebae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yu Guo[Author] Genome-wide association study of brain tau deposition as measured by 18F-flortaucipir positron emission tomography imaging[Title] Neurobiology of Aging[Journal] 2021:2023[Publication Date]'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_info\n",
    "qry = \"\"\n",
    "if pdf_info[\"/Author\"]:\n",
    "    qry += \"{}[Author] \".format(pdf_info[\"/Author\"])\n",
    "if pdf_info[\"/Title\"]:\n",
    "    qry += \"{}[Title] \".format(pdf_info[\"/Title\"])\n",
    "if pdf_info[\"/Subject\"]:\n",
    "    journal = pdf_info[\"/Subject\"].split(\",\")[0]\n",
    "    qry += \"{}[Journal] \".format(journal)\n",
    "    year = re.findall(\"(\\d\\d\\d\\d)\", pdf_info[\"/Subject\"])\n",
    "    if year:\n",
    "        year = int(year[0])\n",
    "        qry += \"{}:{}[Publication Date]\".format(year - 1, year + 1)\n",
    "results = pubmed_search(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "802c026c-92a0-42f4-8308-eaff9cf5f16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36195041']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4bfda11-900a-4bce-83b7-e86d9cb6359b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIdList\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "results[\"IdList\"][\"Year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eed2a3da-50e9-4f03-925a-19abeffc63e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guo2022\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"{}{}\".format(\n",
    "        paper[\"MedlineCitation\"][\"Article\"][\"AuthorList\"][0][\"LastName\"],\n",
    "        paper[\"MedlineCitation\"][\"Article\"][\"ArticleDate\"][0][\"Year\"],\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
